{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WSD with BERT",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXn5oo_-wPkM",
        "colab_type": "code",
        "outputId": "efadc81a-cd97-4351-feb1-86d7e052835f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import os\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 487,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 487
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D4QQxqcxW8t",
        "colab_type": "text"
      },
      "source": [
        "# WordNet: Sense, Definition, Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_xoyJ2bS_AC",
        "colab_type": "code",
        "outputId": "18e6aeba-f281-428f-bfcd-f88211ae9dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#https://pythonprogramming.net/wordnet-nltk-tutorial/\n",
        "word=\"constant\"\n",
        "for i in wn.synsets(word):\n",
        "  print(i,\"\\nDEF: \",i.definition(),\"\\nEXAMPLES\",i.examples())#,wn.synset('book.n.01').examples()"
      ],
      "execution_count": 488,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('constant.n.01') \n",
            "DEF:  a quantity that does not vary \n",
            "EXAMPLES []\n",
            "Synset('constant.n.02') \n",
            "DEF:  a number representing a quantity assumed to have a fixed value in a specified mathematical context \n",
            "EXAMPLES ['the velocity of light is a constant']\n",
            "Synset('changeless.s.02') \n",
            "DEF:  unvarying in nature \n",
            "EXAMPLES ['maintained a constant temperature', 'principles of unvarying validity']\n",
            "Synset('constant.a.02') \n",
            "DEF:  steadfast in purpose or devotion or affection \n",
            "EXAMPLES ['a man constant in adherence to his ideals', 'a constant lover', 'constant as the northern star']\n",
            "Synset('ceaseless.s.01') \n",
            "DEF:  uninterrupted in time and indefinitely long continuing \n",
            "EXAMPLES ['the ceaseless thunder of surf', 'in constant pain', 'night and day we live with the incessant noise of the city', 'the never-ending search for happiness', 'the perpetual struggle to maintain standards in a democracy', \"man's unceasing warfare with drought and isolation\", 'unremitting demands of hunger']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_9Op53gWhc",
        "colab_type": "text"
      },
      "source": [
        "#SemCor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pFiVX7eYDf6",
        "colab_type": "code",
        "outputId": "53f5467e-bff9-45c2-d5dc-e537e6f6fdac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#https://github.com/google-research-datasets/word_sense_disambigation_corpora\n",
        "#https://www.kaggle.com/saeed1507100/word-sense-disambiguation-1\n",
        "#http://www.nltk.org/howto/wsd.html\n",
        "#https://aclweb.org/aclwiki/Word_sense_disambiguation_resources\n",
        "!git clone https://github.com/nvshrao/Temp.git"
      ],
      "execution_count": 489,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Temp' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjccMn2lSLAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parselex(lex):\n",
        "  lemma,lexsn = lex.split(\"%\")\n",
        "  i=lexsn.split(\":\")\n",
        "  pos=[\"_\",\"n\",\"v\",\"a\",\"r\",\"s\"]\n",
        "  pos=pos[int(i[0])]\n",
        "  lex_id = i[2]\n",
        "  return lemma+\".\"+pos+\".\"+lex_id\n",
        "\n",
        "def parseentry(sentence,prints=True):\n",
        "  \"\"\"\n",
        "  returns Sentence, senseid of each word, corresonding sense, all senses of the word, all corresponding definitions\n",
        "  may return error\n",
        "  \"\"\"\n",
        "  sent=[]\n",
        "  sens=[]\n",
        "  for entry in sentence.split(\"\\n\"):\n",
        "    if \"</wf>\" in entry:\n",
        "      word = entry.split('\">')[1].split(\"</wf>\")[0]\n",
        "      sent.append(word)\n",
        "      if \"lemma=\" in entry:\n",
        "        lemma=entry.split('lemma=\"')[1].split('\"')[0]\n",
        "        lexsn=entry.split('lexsn=\"')[1].split('\"')[0]\n",
        "        sense=lemma+\"%\"+lexsn\n",
        "        sens.append(sense)\n",
        "      else:\n",
        "        sens.append(\"_\")\n",
        "    elif \"<punc>\" in entry:\n",
        "      sent.append(entry.strip(\"<punc>\").strip(\"</punc>\"))\n",
        "      sens.append(\"_\")\n",
        "    else:\n",
        "      if prints==True:\n",
        "        print(entry)\n",
        "  syn = [wn.synset(parselex(i))  if \"_\" not in i else \"_\" for i in sens]\n",
        "  syndef = [wn.synset(parselex(i)).definition()  if \"_\" not in i else \"_\" for i in sens]\n",
        "  assert len(sent)==len(syn)\n",
        "  other=[]\n",
        "  otherdef=[]\n",
        "  #sentence, senses= parseentry(sents[3])\n",
        "  for i,word in enumerate(sent):\n",
        "    if str(syn[i])!=\"_\":\n",
        "      other_senses=wn.synsets(word)\n",
        "      other.append(other_senses)\n",
        "      otherdef.append([i.definition() for i in other_senses])\n",
        "    else:\n",
        "      other.append(\"_\")\n",
        "      otherdef.append(\"_\")\n",
        "  return sent,syn,syndef,other,otherdef\n",
        "\n",
        "def readfile(filename,prints=False):\n",
        "  file=open(filename).read()\n",
        "  paras=file.split(\"<p pnum=\")\n",
        "  if prints==True:\n",
        "    print(len(paras))\n",
        "  sents=[i.split(\"<s snum=\") for i in paras]\n",
        "  sents = [j for i in sents for j in i]# Ignore first two sents mostly, check last one\n",
        "  if prints==True:\n",
        "    print(len(sents))\n",
        "  sents = [i for i in sents if \"</wf>\" in i]# only those with at least a word\n",
        "  if prints==True:\n",
        "    print(len(sents))\n",
        "  return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yTaf0Vno3MI",
        "colab_type": "code",
        "outputId": "f6d66dbd-5af9-43a2-e7cb-55d0d11da186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = \"./Temp/data/semcor/brown1/tagfiles/\"\n",
        "brown1files = os.listdir(path)\n",
        "f = [readfile(path+i) for i in brown1files]\n",
        "brown1 = [j for i in f for j in i]\n",
        "\n",
        "path = \"./Temp/data/semcor/brown2/tagfiles/\"\n",
        "brown2files = os.listdir(path)\n",
        "f = [readfile(path+i) for i in brown2files]\n",
        "brown2 = [j for i in f for j in i]\n",
        "\n",
        "path = \"./Temp/data/semcor/brownv/tagfiles/\"\n",
        "brownvfiles = os.listdir(path)\n",
        "f = [readfile(path+i) for i in brownvfiles]\n",
        "brownv = [j for i in f for j in i]\n",
        "\n",
        "allsentences=brown1+brown2+brownv\n",
        "print(len(brown1),len(brown2),len(brownv),len(allsentences))"
      ],
      "execution_count": 491,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11178 8954 17036 37168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GItSBCmOcnC7",
        "colab_type": "code",
        "outputId": "3f9d0819-4290-4681-e8b5-783a7655172a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "errors=[]\n",
        "sentences=[]\n",
        "synset=[]\n",
        "sense=[]\n",
        "allsynsets=[]\n",
        "allsenses=[]\n",
        "for i,j in enumerate(allsentences):\n",
        "  try:\n",
        "    a,b,c,d,e = parseentry(j,prints=False)\n",
        "    sentences.append(a)\n",
        "    synset.append(b)\n",
        "    sense.append(c)\n",
        "    allsynsets.append(d)\n",
        "    allsenses.append(e)\n",
        "  except:\n",
        "    errors.append(i)\n",
        "assert len(sentences) == len(synset) == len(sense) == len(allsynsets) == len(allsenses)\n",
        "len(errors),len(allsentences)"
      ],
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3820, 37168)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 492
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpMxsyY2t-0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wn.synsets(\"historical\")\n",
        "#wn.synset('historical.s.00')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyhbO4Djt-w5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "39fc38b6-6069-4ce3-8aaa-b0752bbc7985"
      },
      "source": [
        "data = {'sentences':sentences, 'synset':synset,\"sense\":sense,\"allsynsets\":allsynsets, \"allsenses\":allsenses}\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 494,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>synset</th>\n",
              "      <th>sense</th>\n",
              "      <th>allsynsets</th>\n",
              "      <th>allsenses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Andy, did, not, see, the, newspapers, the, ne...</td>\n",
              "      <td>[Synset('person.n.03'), _, Synset('not.r.01'),...</td>\n",
              "      <td>[a grammatical category used in the classifica...</td>\n",
              "      <td>[[], _, [Synset('not.r.01')], [Synset('see.n.0...</td>\n",
              "      <td>[[], _, [negation of a word or group of words]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Someone, on, his, staff, -, he, suspected, it...</td>\n",
              "      <td>[Synset('person.n.01'), _, _, Synset('staff.n....</td>\n",
              "      <td>[a human being, _, _, personnel who assist the...</td>\n",
              "      <td>[[Synset('person.n.01')], _, _, [Synset('staff...</td>\n",
              "      <td>[[a human being], _, _, [personnel who assist ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Memory, flooded, him, the, instant, he, opene...</td>\n",
              "      <td>[Synset('memory.n.02'), Synset('flood.v.04'), ...</td>\n",
              "      <td>[the cognitive processes whereby past experien...</td>\n",
              "      <td>[[Synset('memory.n.01'), Synset('memory.n.02')...</td>\n",
              "      <td>[[something that is remembered, the cognitive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Outside, his, window, bloomed, a, beautiful, ...</td>\n",
              "      <td>[_, _, Synset('window.n.08'), Synset('bloom.v....</td>\n",
              "      <td>[_, _, (computer science) a rectangular part o...</td>\n",
              "      <td>[_, _, [Synset('window.n.01'), Synset('window....</td>\n",
              "      <td>[_, _, [a framework of wood or metal that cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Presumably, the, same, sun, was, shining, upo...</td>\n",
              "      <td>[Synset('presumably.r.01'), _, Synset('same.a....</td>\n",
              "      <td>[by reasonable assumption, _, closely similar ...</td>\n",
              "      <td>[[Synset('presumably.r.01')], _, [Synset('lapp...</td>\n",
              "      <td>[[by reasonable assumption], _, [a member of a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences  ...                                          allsenses\n",
              "0  [Andy, did, not, see, the, newspapers, the, ne...  ...  [[], _, [negation of a word or group of words]...\n",
              "1  [Someone, on, his, staff, -, he, suspected, it...  ...  [[a human being], _, _, [personnel who assist ...\n",
              "2  [Memory, flooded, him, the, instant, he, opene...  ...  [[something that is remembered, the cognitive ...\n",
              "3  [Outside, his, window, bloomed, a, beautiful, ...  ...  [_, _, [a framework of wood or metal that cont...\n",
              "4  [Presumably, the, same, sun, was, shining, upo...  ...  [[by reasonable assumption], _, [a member of a...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 494
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYb6wXKfBBCn",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Dm7hRXr91w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "d10b70c0-fe3a-4918-ab22-830ea79b0aba"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.special import expit\n",
        "import torch\n",
        "!pip install tensorflow-gpu\n",
        "!pip install transformers\n",
        "import transformers"
      ],
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (45.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.18)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.18)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHOk4er4N6Gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class = transformers.BertModel\n",
        "tokenizer_class = transformers.BertTokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
        "bertmodel = model_class.from_pretrained(model_name,output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "#padded_seq=tokenizer.encode(\"Hi My name is anvesh\")\n",
        "#input_ids = torch.tensor(np.array(padded_seq))\n",
        "#with torch.no_grad():\n",
        "#    torch_output = model(input_ids)\n",
        "#last_hidden_state,pooler_output,hidden_states,attentions = torch_output\n",
        "#last_hidden_state.shape, pooler_output.shape, torch.stack(hidden_states).shape, torch.stack(attentions).shape\n",
        "#assert (hidden_states[-1] == last_hidden_state).all()\n",
        "\n",
        "T=30\n",
        "def tknz(sentence,T=30):\n",
        "  return tokenizer.encode(sentence,add_special_tokens = True,max_length = T,pad_to_max_length=True)\n",
        "\n",
        "def get_representation(sentence,model=bertmodel):\n",
        "  if type(sentence) is list:\n",
        "    sentence=\" \".join(sentence)\n",
        "  assert type(sentence)==str\n",
        "  padded_seq = tknz(sentence)\n",
        "  num_pads = padded_seq.count(0) if padded_seq.count(0) != 0 else -T-1#  -0 wont work if no pads\n",
        "  padded_seq=[padded_seq]\n",
        "  input_ids = torch.tensor(np.array(padded_seq))\n",
        "  with torch.no_grad():\n",
        "    torch_output = model(input_ids)\n",
        "  last_hidden_state, pooler_output, hidden_states, attentions = torch_output\n",
        "  #return np.array(last_hidden_state)[0][:-num_pads][1:-1] # SEP and CLS are to be ignored\n",
        "  return np.array(last_hidden_state)[0][:-num_pads] #because only 1 sentence and avoid padding representations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yboir1MSYfiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def findword(word,sentence):\n",
        "  \"\"\" Will only work if word hasn't been split in more than two tokens \"\"\"\n",
        "  word=word.lower()\n",
        "  flag=0\n",
        "  s = \" \".join(sentence).replace(\"_\",\" \")\n",
        "  en = tokenizer.encode(s,max_length = 30,pad_to_max_length=True)\n",
        "  bert_tokenized_sentence=tokenizer.convert_ids_to_tokens(en)\n",
        "  if word in bert_tokenized_sentence:\n",
        "    return [bert_tokenized_sentence.index(word)]\n",
        "  else:\n",
        "    for i,w in enumerate(bert_tokenized_sentence):\n",
        "      if \"##\" in w:\n",
        "        if bert_tokenized_sentence[i-1]+bert_tokenized_sentence[i].replace(\"##\",\"\") == word:\n",
        "          return [i-1,i]\n",
        "  return -1# could not find\n",
        "\n",
        "def WSD(word,sentence):\n",
        "  \"\"\"\n",
        "  sentence needs to be a list of tokens and word needs to be present in the list\n",
        "  return correct wn sense and defination\n",
        "  \"\"\"\n",
        "  word_indices = findword(word,sentence)\n",
        "  if word_indices == -1:\n",
        "    print(\"Couldn't find word in sentence.\")\n",
        "    return\n",
        "  syns= wn.synsets(word)\n",
        "  if len(syns)<=1:\n",
        "    print(\"Word does not have enough senses.\")\n",
        "    return \n",
        "  all_senses = [i.definition() for i in syns]\n",
        "  whole = get_representation(sentence)\n",
        "  word_vector = np.mean(np.take(whole,word_indices,axis=0),axis=0)\n",
        "  all_senses_vectors = [np.mean(get_representation(i),axis=0) for i in all_senses]\n",
        "  distances = [np.linalg.norm(word_vector-i) for i in all_senses_vectors]\n",
        "  minindex = np.argmin(distances)\n",
        "  assert(len(distances)==len(syns))\n",
        "  sorted_syns = [x for _,x in sorted(zip(distances,syns))]\n",
        "  return sorted_syns,sorted_syns[0].definition() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wag4edBuwdjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "9d73fc99-4a3d-4935-d5d6-7c312ef97928"
      },
      "source": [
        "word ='bank'\n",
        "sentence = ['He', 'sat', 'at', 'the',\"river\",'bank',\".\"]\n",
        "WSD(word,sentence)"
      ],
      "execution_count": 498,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Synset('bank.n.01'),\n",
              "  Synset('depository_financial_institution.n.01'),\n",
              "  Synset('deposit.v.02'),\n",
              "  Synset('bank.n.06'),\n",
              "  Synset('bank.n.09'),\n",
              "  Synset('bank.v.02'),\n",
              "  Synset('savings_bank.n.02'),\n",
              "  Synset('bank.v.03'),\n",
              "  Synset('bank.v.05'),\n",
              "  Synset('bank.n.10'),\n",
              "  Synset('bank.n.03'),\n",
              "  Synset('bank.v.04'),\n",
              "  Synset('bank.n.05'),\n",
              "  Synset('bank.n.04'),\n",
              "  Synset('bank.v.07'),\n",
              "  Synset('bank.n.07'),\n",
              "  Synset('trust.v.01'),\n",
              "  Synset('bank.v.01')],\n",
              " 'sloping land (especially the slope beside a body of water)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 498
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7r4F1jTzvTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "94ec79c0-095b-45f9-f5e9-baadbce804f4"
      },
      "source": [
        "word ='bank'\n",
        "sentence = ['He', 'worked', \"as\",\"a\",\"cashier\",'at', 'the',\"bank\",\".\"]\n",
        "WSD(word,sentence)"
      ],
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Synset('depository_financial_institution.n.01'),\n",
              "  Synset('bank.v.03'),\n",
              "  Synset('deposit.v.02'),\n",
              "  Synset('bank.n.06'),\n",
              "  Synset('savings_bank.n.02'),\n",
              "  Synset('bank.v.05'),\n",
              "  Synset('bank.n.09'),\n",
              "  Synset('bank.v.02'),\n",
              "  Synset('bank.n.01'),\n",
              "  Synset('bank.v.04'),\n",
              "  Synset('bank.n.05'),\n",
              "  Synset('bank.n.04'),\n",
              "  Synset('bank.n.07'),\n",
              "  Synset('bank.n.10'),\n",
              "  Synset('bank.v.07'),\n",
              "  Synset('trust.v.01'),\n",
              "  Synset('bank.n.03'),\n",
              "  Synset('bank.v.01')],\n",
              " 'a financial institution that accepts deposits and channels the money into lending activities')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 499
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeEOuf4p2wj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "954308be-0f2b-4b98-dfcc-16057e334829"
      },
      "source": [
        "word ='currents'\n",
        "sentence = ['The', 'currents', \"in\",\"the\",\"river\",\"were\",\"strong\",\".\"]\n",
        "WSD(word,sentence)"
      ],
      "execution_count": 500,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Synset('stream.n.02'), Synset('current.n.01'), Synset('current.n.02')],\n",
              " 'dominant course (suggestive of running water) of successive events or ideas')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 500
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjy3oOB4CFBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ac06aff0-218a-427a-d855-9db5a03f973c"
      },
      "source": [
        "word ='current'\n",
        "sentence = ['current', 'events', \"show\",\"the\",\"pandemic\",\"clearly\",\".\"]\n",
        "WSD(word,sentence)"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Synset('current.a.01'),\n",
              "  Synset('current.n.01'),\n",
              "  Synset('stream.n.02'),\n",
              "  Synset('current.n.02')],\n",
              " 'occurring in or belonging to the present time')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 501
        }
      ]
    }
  ]
}